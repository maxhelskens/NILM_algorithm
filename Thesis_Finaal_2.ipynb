{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from scipy.signal import butter\n",
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 16,8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the data you want to check ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data.csv', index_col=0)\n",
    "df.index = pd.to_datetime(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blockdetection Functions #\n",
    "\n",
    " * Z-normalisation\n",
    " * PAA-transform\n",
    " * SAX-transform\n",
    " * Actual blockdetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def znormalization(ts):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            ts (np.array)\n",
    "\n",
    "        Returns:\n",
    "            (np.array)\n",
    "    \"\"\"\n",
    "    mus = ts.mean(axis = 0)\n",
    "    stds = ts.std(axis = 0)\n",
    "    return (ts - mus) / stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def paa_transform(ts, n_pieces):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            ts (np.array)\n",
    "            n_pieces (int) : M equally sized piecies into which the original ts is splitted\n",
    "            \n",
    "        Returns:\n",
    "            (np.array) : ts's paa transformation\n",
    "    \"\"\"\n",
    "    splitted = np.array_split(ts, n_pieces) ## along columns as we want\n",
    "    return np.asarray(map(lambda xs: xs.mean(axis = 0), splitted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sax_transform(ts, n_pieces, level_nr):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            ts (np.array) \n",
    "            n_pieces (int) : number of segments in paa transformation\n",
    "            level_nr (int) : the amount of levels\n",
    "        \n",
    "        Returns: \n",
    "            (np.array) : ts's sax transformation\n",
    "            \n",
    "    \"\"\"\n",
    "    from scipy.stats import norm\n",
    "    \n",
    "    levels = np.arange(level_nr)\n",
    "    \n",
    "    thrholds = norm.ppf(np.linspace(1./level_nr, \n",
    "                                    1-1./level_nr, \n",
    "                                    level_nr-1))\n",
    "    def translate(ts_values):\n",
    "        return np.asarray([(levels[0] if ts_value < thrholds[0]\n",
    "                else (levels[-1] if ts_value > thrholds[-1]\n",
    "                      else levels[np.where(thrholds <= ts_value)[0][-1]+1]))\n",
    "                           for ts_value in ts_values])\n",
    "    \n",
    "    paa_ts = paa_transform(znormalization(ts), n_pieces)\n",
    "    return np.apply_along_axis(translate, 0, paa_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sax_rm_shortage_new(dataframe, factor):\n",
    "    \n",
    "#     dataframe = dataframe.rolling(window=6,center=False).median()\n",
    "#     dataframe.index = dataframe.index - pd.Timedelta(minutes=3)\n",
    "#     dataframe = dataframe.dropna()\n",
    "    \n",
    "    df_w_val = []\n",
    "    max_lvl = dataframe.max()[0]\n",
    "    min_lvl = dataframe.min()[0]\n",
    "    \n",
    "    difference = max_lvl - min_lvl\n",
    "    lvl_niv = round(difference/factor)\n",
    "    \n",
    "    for i in range(0, dataframe.index.size):\n",
    "        current_val = dataframe[dataframe.columns[0]][i] #current value processed\n",
    "        ratio = current_val/max_lvl\n",
    "        assign_lvl = round(ratio*(factor))\n",
    "        \n",
    "        df_w_val.append(assign_lvl)\n",
    "    \n",
    "    df_w_val = pd.DataFrame(index=dataframe.index, data= df_w_val , columns=['repeatedValues'])\n",
    "    df_w_val.index = dataframe.index\n",
    "    return df_w_val, dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeArray(array1):\n",
    "    \"\"\"\n",
    "        Turn sax-transform letters back into numbers\n",
    "        \n",
    "        Args:\n",
    "            array1 (pd.DataFrame)\n",
    "        \n",
    "        Returns:\n",
    "            df (pd.DataFrame)\n",
    "        \n",
    "    \"\"\"\n",
    "    arrayValues=[]    \n",
    "    for i in range(0, array1.size-1):\n",
    "        if(i<array1.size):\n",
    "             arrayValues.append(ord(array1.iloc[i])-97)\n",
    "    df=pd.DataFrame(arrayValues)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manhattanDistance(s1,s2):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            s1 (pd.DataFrame)\n",
    "            s2 (pd.DataFrame)\n",
    "        \n",
    "        Returns:\n",
    "            dist (Float) : Measure of the similarity of s1 and s2\n",
    "    \"\"\"\n",
    "    dist = distance.cityblock(s1,s2)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_max_consumption(start, stop, parent, threshold=4.0):\n",
    "    \"\"\"\n",
    "        Check if the found block has a certain minimum consumption\n",
    "        \n",
    "        Args:\n",
    "            start (timestamp) : date & time when the found block started\n",
    "            stop (timestamp) : date & time when the found block ended\n",
    "            parent (pd.DataFrame) : The parent of the found block\n",
    "            threshold (int) : Minimum desired consumption\n",
    "            \n",
    "        Returns:\n",
    "            True/ False\n",
    "    \"\"\"\n",
    "    \n",
    "    head = pd.Timestamp(start)\n",
    "    tail = pd.Timestamp(stop)\n",
    "    temp = parent.ix[head:tail]\n",
    "    \n",
    "    if float(temp.values.max() - temp.values.min()) < threshold:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_blocks(df_parent, row_parent, blocks, df_original, df_rm, max_length, max_consumption, first_time = True):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            df_parent (pd.DataFrame) : parent block\n",
    "            row_parent (int) : index of the parent block\n",
    "            blocks (pd.DataFrame) : List of all previously found blocks\n",
    "            df_original (pd.DataFrame) : sax-equivalent\n",
    "            first_time (Boolean)\n",
    "            df_rm (pd.DataFrame) : rolling mean of orriginal\n",
    "            \n",
    "        Returns:\n",
    "            blocks (pd.DataFrame) : List of all previously found blocks\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    min_lvl = 0\n",
    "    start = False\n",
    "    \n",
    "    block_start = 0\n",
    "    block_end = 0\n",
    "    index_low = -1\n",
    "    \n",
    "    nr_of_blocks_start = blocks.size\n",
    "    \n",
    "    nested = row_parent\n",
    "    \n",
    "    if first_time:\n",
    "        start = True\n",
    "        min_lvl = df_parent['repeatedValues'].min()\n",
    "        \n",
    "    #append last value again\n",
    "    post_index = df_parent.index[-1] + pd.Timedelta(minutes=1)\n",
    "\n",
    "    temp = pd.DataFrame(data=[df_parent.iloc[-1]], index=[post_index], columns=['repeatedValues'])\n",
    "    df_parent = df_parent.append(temp)\n",
    "\n",
    "    df_parent.sort_index(inplace=True)\n",
    "    \n",
    "    for i in range(1, df_parent.size-1):\n",
    "        if start == False and df_parent['repeatedValues'].iloc[i - 1] - df_parent['repeatedValues'].iloc[i] == 0:\n",
    "            min_lvl = df_parent['repeatedValues'].iloc[i]\n",
    "            start = True\n",
    "        \n",
    "        if start == True and df_parent['repeatedValues'].iloc[i] > min_lvl and df_parent['repeatedValues'].iloc[i - 1] == min_lvl:\n",
    "            block_start = df_parent.index[i - 1]\n",
    "            \n",
    "        if start == True and block_start != 0 and df_parent['repeatedValues'].iloc[i] <= min_lvl:\n",
    "            block_end = df_parent.index[i]\n",
    "            \n",
    "            #Match if the length is more than 10% less than the parent\n",
    "            temp = pd.DataFrame(data=[[str(block_start), str(block_end), nested]], columns=['start', 'stop', 'nested_in'])\n",
    "            \n",
    "            if float(df_parent.index.size - df_parent.ix[block_start : block_end].index.size) / float(df_parent.index.size) > 0.10:\n",
    "                # check if block is longer than 15 min\n",
    "                if(block_end - block_start > pd.Timedelta(seconds = max_length)):\n",
    "                    # check max consumption is greater than threshold\n",
    "                    if check_max_consumption(start=block_start, stop=block_end, parent=df_rm, threshold=max_consumption):\n",
    "                        # Check for duplicates\n",
    "                        check, blocks = check_duplicate(blocks, block_start, block_end)\n",
    "                        if check:\n",
    "                            #print [block_start, block_end]\n",
    "                            blocks = blocks.append(temp, ignore_index=True)\n",
    "            \n",
    "            df_parent2 = df_original.ix[block_start : block_end]\n",
    "            row_parent = blocks.index.size - 1\n",
    "            blocks = find_blocks(df_parent2, row_parent, blocks, df_original, df_rm, max_length, max_consumption, False)\n",
    "            blocks =  blocks.drop_duplicates(subset=['start', 'stop'], keep= 'first')\n",
    "            block_start = 0\n",
    "            block_end = 0\n",
    "            \n",
    "        if start == True and block_start == 0 and df_parent['repeatedValues'].iloc[i] < min_lvl:\n",
    "            #Lvl dropped below min value so the min value was not assigned properly\n",
    "            # Store the timestamp where the value is the lowest for the first time.\n",
    "            if index_low == -1:\n",
    "                index_low = i\n",
    "                \n",
    "            if df_parent['repeatedValues'].iloc[index_low] > df_parent['repeatedValues'].iloc[i]:\n",
    "                index_low = i\n",
    "            \n",
    "            if df_parent['repeatedValues'].iloc[i+1] > df_parent['repeatedValues'].iloc[i]:\n",
    "                \n",
    "                #Match if the length is more than 10% less than the parent\n",
    "                temp = pd.DataFrame(data=[[str(df_parent.index[0]), str(df_parent.index[index_low]), nested]], columns=['start', 'stop', 'nested_in'])\n",
    "            \n",
    "                if float(df_parent.index.size - df_parent.ix[df_parent.index[0] : df_parent.index[index_low]].index.size) / float(df_parent.index.size) > 0.10:\n",
    "                    # check if block is longer than 15 min\n",
    "                    if(df_parent.index[index_low]-df_parent.index[0] > pd.Timedelta(seconds = max_length)):\n",
    "                        # check max consumption is greater than threshold\n",
    "                        if check_max_consumption(start=df_parent.index[0], stop=df_parent.index[index_low], parent=df_rm, threshold=max_consumption):\n",
    "                            # check for duplicates\n",
    "                            check, blocks = check_duplicate(blocks, df_parent.index[0], df_parent.index[index_low])\n",
    "                            if check:\n",
    "                                blocks = blocks.append(temp, ignore_index=True)\n",
    "                \n",
    "                min_lvl = df_parent['repeatedValues'].iloc[index_low]\n",
    "                \n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyValidationError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sax_rm_shortage(dataframe):\n",
    "    \"\"\"\n",
    "        Append values to the sax result to make up for the lost values in the transformation\n",
    "        \n",
    "        Args:\n",
    "            dataframe (pd.DataFrame) : original consumption data\n",
    "        \n",
    "        Returns:\n",
    "            df_repeatedValues (pd.DataFrame) : sax transform\n",
    "            df_rm (pd.DataFrame) : the rolling mean of the original dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    if dataframe.size <= 0:\n",
    "        raise MyValidationError(\"Dataframe must have a size bigger than 0\")\n",
    "    \n",
    "        return None, None\n",
    "\n",
    "    df_rm = dataframe.rolling(window=6,center=False).median()\n",
    "    df_rm.index = df_rm.index - pd.Timedelta(minutes=3)\n",
    "    df_rm = df_rm.dropna()\n",
    "    \n",
    "    #saxtransform\n",
    "    lettersToNumSax = pd.DataFrame(sax_transform(df_rm, df_rm.size/3, 16))\n",
    "\n",
    "    repeatedValues=np.repeat(lettersToNumSax.as_matrix(),3) #expects an array\n",
    "    #make sure the dataframes are of the same length to join the data and the index\n",
    "    shortage = df_rm.size - repeatedValues.size\n",
    "    if shortage > 0:\n",
    "        repeatedValues = np.append(repeatedValues, [repeatedValues[-1]]*shortage)\n",
    "\n",
    "    elif shortage < 0:\n",
    "        repeatedValues = repeatedValues[0:repeatedValues.size + shortage]\n",
    "    \n",
    "    shortage = df_rm.size - repeatedValues.size\n",
    "    \n",
    "    df_repeatedValues = pd.DataFrame(index=df_rm.index, data= repeatedValues, columns=['repeatedValues'])\n",
    "    return df_repeatedValues, df_rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new block detection\n",
    "def blockdetection_new(df_repeatedValues, df_rm, seconds, max_lvl, show=False):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            df_repeatedValues (pd.DataFrame) : sax transform\n",
    "            df_rm (pd.DataFrame) : the rolling mean of the original datcaframe\n",
    "            show (Boolean) : plot the result or not\n",
    "        \n",
    "        Returns:\n",
    "            df_rm (pd.DataFrame) : the rolling mean of the original dataframe\n",
    "            blocks (pd.DataFrame) : List of all found blocks\n",
    "    \"\"\"    \n",
    "\n",
    "    #pre- and append a zero to make sure the sample starts and ends with the same value\n",
    "    for i in range(0, 2):\n",
    "        pre_index = df_repeatedValues.index[0] - pd.Timedelta(minutes=1)\n",
    "        post_index = df_repeatedValues.index[-1] + pd.Timedelta(minutes=1)\n",
    "\n",
    "        temp = pd.DataFrame(data=[df_repeatedValues.min()], index=[pre_index], columns=['repeatedValues'])\n",
    "        df_repeatedValues = df_repeatedValues.append(temp)\n",
    "\n",
    "        temp = pd.DataFrame(data=[df_repeatedValues.min()], index=[post_index], columns=['repeatedValues'])\n",
    "        df_repeatedValues = df_repeatedValues.append(temp)\n",
    "\n",
    "        df_repeatedValues.sort_index(inplace=True)\n",
    "    \n",
    "    #Do the actual searching\n",
    "    blocks = pd.DataFrame(columns=['start', 'stop', 'nested_in'])\n",
    "    blocks = find_blocks(df_repeatedValues, -1, blocks, df_repeatedValues, df_rm, seconds, max_lvl)\n",
    "    \n",
    "    if show:\n",
    "    \n",
    "        #plot the dataframe with all the blocks\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(df_rm.index, df_rm, color='grey')\n",
    "\n",
    "        colors = 100*['red', 'blue', 'orange', 'green', 'yellow']\n",
    "\n",
    "\n",
    "        for i in range(0, blocks.index.size):\n",
    "            ax.axvspan(str(pd.Timestamp(blocks['start'].iloc[i])), str(pd.Timestamp(blocks['stop'].iloc[i])), alpha=0.1, color=colors[3])\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    #return the rolling mean equivalent and the blocks dataframe\n",
    "    return df_rm, blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern recognition functions #\n",
    "\n",
    "* Distance metric (Manhattan)\n",
    "* Clustering\n",
    "* Clean up results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keep_local_minima(score_list):\n",
    "    \"\"\"\n",
    "        This function keeps the local minima in the manhattan scores\n",
    "        \n",
    "        Args:\n",
    "            score_list (np.array) : list of all manhattan scores of one sample looped over another\n",
    "            \n",
    "        Returns:\n",
    "            (list) : only the minima\n",
    "    \"\"\"\n",
    "    local_minima_scores = [row[0] for row in score_list]\n",
    "    arr_n = np.asarray(local_minima_scores)\n",
    "    df = pd.DataFrame(score_list, columns=[\"score\",\"start\",\"stop\",\"found in block\"])\n",
    "    tr_f = np.r_[True, arr_n[1:] < arr_n[:-1]] & np.r_[arr_n[:-1] < arr_n[1:], True]\n",
    "    tr_f_list = tr_f.tolist()\n",
    "    df_local_minima = df.loc[tr_f_list]\n",
    "    \n",
    "    #return df_local_minima.as_matrix(columns=[df_local_minima.columns])\n",
    "    return df_local_minima.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_score(minima_list, score):\n",
    "    \"\"\"\n",
    "        Return the id of a specific score\n",
    "        \n",
    "        Args:\n",
    "            minima_list (list)\n",
    "            score (Float) : score to find\n",
    "            \n",
    "        Returns:\n",
    "            i (int) : id of the score\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(0, len(minima_list)):\n",
    "        if minima_list[i][0] == score:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ts_correction(local_minima_manhattan):\n",
    "    \"\"\"\n",
    "        If local minima blocks have an overlap the one with the best score is kept\n",
    "        \n",
    "        Args:\n",
    "            local_minima_manhattan (pd.DataFrame) : contains matching blocks\n",
    "            \n",
    "        Returns:\n",
    "            result_list_n (pd.DataFrame) : contains matching blocks without overlap\n",
    "    \"\"\"\n",
    "    \n",
    "    result_list_n=[]\n",
    "    max_value = 0\n",
    "    \n",
    "    for i in range(0, local_minima_manhattan.index.size):\n",
    "        current_row = local_minima_manhattan.iloc[i]\n",
    "        if(i==0):\n",
    "            result_list_n.append(current_row)\n",
    "        else:\n",
    "            previous_row  = result_list_n[len(result_list_n)-1]\n",
    "            if(current_row['start'] >= previous_row['start']) and (current_row['start'] <= previous_row['stop']):\n",
    "                #Inside\n",
    "                if(current_row[0] <= previous_row[0]):\n",
    "                    ind = find_score(result_list_n, previous_row[0])\n",
    "                    result_list_n[ind] = current_row\n",
    "            #Outside\n",
    "            else:\n",
    "                result_list_n.append(current_row)\n",
    "    \n",
    "    return result_list_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_threshold_value(df_result, nr_of_best_values, factor):\n",
    "    \"\"\"\n",
    "        A thresholdvalue is calculated based on the x best matches\n",
    "        \n",
    "        Args:\n",
    "            df_result (pd.DataFrame) : contains allmatches for 1 block\n",
    "            nr_of_best_values (int)\n",
    "            factor (Float)\n",
    "            \n",
    "        Returns\n",
    "            (pd.DataFrame) : The thresholded list\n",
    "            \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    if df_result.index.size < nr_of_best_values:\n",
    "        nr_of_best_values = 1\n",
    "    \n",
    "    list_of_best_values=[]\n",
    "    i = 0\n",
    "    for i in range(0, nr_of_best_values):\n",
    "        list_of_best_values.append(df_result.iloc[i])\n",
    "    list_of_best_values=pd.DataFrame(list_of_best_values)\n",
    "    mean=list_of_best_values.mean()\n",
    "    threshold=mean*factor\n",
    "    \n",
    "    return df_result.loc[df_result['score']<=threshold[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split(original, event_list, parts):\n",
    "    \"\"\"\n",
    "        Split the resulting matches in an amount of parts and store their means\n",
    "        \n",
    "        Args:\n",
    "            original (pd.DataFrame) : block we were looking for\n",
    "            event_list (pd.DataFrame) : contains allmatches for the original block\n",
    "            parts (int)\n",
    "            \n",
    "        Returns:\n",
    "            (pd.DataFrame)\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = []\n",
    "    for i in range(0, parts):\n",
    "        cols.extend(['part_' + str(i)])\n",
    "        \n",
    "    cols.extend(['start_value'])\n",
    "    cols.extend(['stop_value'])\n",
    "    \n",
    "    parts_df = pd.DataFrame(columns=cols, index=event_list.index)\n",
    "    \n",
    "    for i in range(0, event_list.index.size):\n",
    "        part_length = (event_list['stop'].iloc[i] - event_list['start'].iloc[i])/parts\n",
    "        \n",
    "        prev_end_time = event_list['start'].iloc[i]\n",
    "        for j in range(0, parts):\n",
    "            part_median = original.ix[prev_end_time: prev_end_time + part_length].mean().iloc[0]\n",
    "            prev_end_time = prev_end_time + part_length\n",
    "            index = event_list.index[i]\n",
    "            \n",
    "            parts_df.set_value(index, 'part_' + str(j), part_median)\n",
    "        \n",
    "        parts_df.set_value(event_list.index[i], 'start_value', original[original.columns[0]].loc[event_list['start'].iloc[i]])\n",
    "        parts_df.set_value(event_list.index[i], 'stop_value', original[original.columns[0]].loc[event_list['stop'].iloc[i]])\n",
    "            \n",
    "    split_list = pd.concat([event_list, parts_df],axis=1)\n",
    "    \n",
    "    return split_list.sort_values('score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster(matches, parts):\n",
    "    \"\"\"\n",
    "        Cluster best matches to label them as being a true match or not\n",
    "        \n",
    "        Args:\n",
    "            matches (pd.DataFrame)\n",
    "            parts (int)\n",
    "            \n",
    "        Returns:\n",
    "            (pd.DataFrame)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    cluster_data = []\n",
    "    matches = matches.fillna(0)\n",
    "    cluster_data = [(matches['score']/matches['score'].max()).as_matrix()]\n",
    "    for i in range(0, parts):\n",
    "        cluster_data.extend([(matches['part_' + str(i)]/matches['part_' + str(i)].max()).as_matrix()])\n",
    "        \n",
    "    cluster_data.extend([(matches['start_value']/matches['start_value'].max()).as_matrix()])\n",
    "    cluster_data.extend([(matches['stop_value']/matches['stop_value'].max()).as_matrix()])\n",
    "        \n",
    "    df_result = pd.DataFrame(data=cluster_data).transpose()\n",
    "    df_result = df_result.fillna(0) #Reomve NaN values   \n",
    "    \n",
    "    result=df_result.as_matrix()\n",
    "    \n",
    "    ms = MeanShift()\n",
    "    ms.fit(result)\n",
    "    labels = ms.labels_\n",
    "    cluster_centers = ms.cluster_centers_\n",
    "    n_cluster = len(np.unique(labels))\n",
    "    colors = 10*['g.', 'r.', 'c.', 'b.', 'k.', 'y.', 'm.']\n",
    "    \n",
    "    try:\n",
    "        del split_matches['Type']\n",
    "        matches.insert(loc=0, column='Type', value=labels)\n",
    "    except Exception as e:\n",
    "        matches.insert(loc=0, column='Type', value=labels)\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare(df_to_find, df_to_check, manhattan_list, block_index, factor=1.5):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            df_to_find (pd.DataFrame): The block you are looking for.\n",
    "            df_to_check (DataFrame): The block to which you are comparing.\n",
    "            manhattan_list (array): list of manhattan scores for df_to_find.\n",
    "\n",
    "        Returns:\n",
    "            result_list (array): List of manhattan scores\n",
    "    \"\"\"\n",
    "    result_list = []\n",
    "    j = 0\n",
    "    for i in range(0, df_to_check.index.size - df_to_find.index.size + 1):\n",
    "        frame = df_to_check[i : i + df_to_find.index.size]\n",
    "#         print \"new_frame\"\n",
    "        if (not frame.max()[0] > df_to_find.max()[0]*factor) or (factor ==0): #Extra thresholding\n",
    "            manhattan_distance = manhattanDistance(frame, df_to_find)\n",
    "            start_ts = frame.index[0]\n",
    "            stop_ts = frame.index[-1]\n",
    "            if (j==0) or (manhattan_distance != result_list[len(result_list)-1][0]):\n",
    "                result_list.append([manhattan_distance, start_ts, stop_ts, block_index])\n",
    "                j = j+1\n",
    "    if len(result_list) > 0:\n",
    "        result_list = keep_local_minima(result_list)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_all(index, blocks, df_rm, block_type, df_types):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            index (int) : index of thec block you are looking for\n",
    "            blocks (pd.DataFrame) : list with all found blocks\n",
    "            df_rm (pd.DataFrame) : rolling mean of the original consumption\n",
    "            block_type (int) : Type assigned to all matches of the block you are looking for\n",
    "            df_types (pd.DataFrame) : list with all types\n",
    "            \n",
    "        Returns:\n",
    "            block_type + 1 (int)\n",
    "            blocks (pd.DataFrame)\n",
    "            df_types (pd.DataFrame)\n",
    "    \"\"\"\n",
    "    \n",
    "    start_to_find = blocks['start'].iloc[index]\n",
    "    stop_to_find = blocks['stop'].iloc[index]\n",
    "    df_to_find = df_rm.ix[pd.Timestamp(start_to_find) - pd.Timedelta(minutes =0):pd.Timestamp(stop_to_find) + pd.Timedelta(minutes =0)]\n",
    "#     df_to_find =  df_to_find[df_to_find['Watt'] > 1]\n",
    "    \n",
    "    manhattan_list = []\n",
    "    \n",
    "    for i in range(0, blocks.index.size):\n",
    "        if index != i and blocks['nested_in'].iloc[i] == -1:\n",
    "            \n",
    "            start_to_check = blocks['start'].iloc[i]\n",
    "            stop_to_check = blocks['stop'].iloc[i]\n",
    "            df_to_check = df_rm.ix[pd.Timestamp(start_to_check) - pd.Timedelta(minutes = 0):pd.Timestamp(stop_to_check) + pd.Timedelta(minutes = 3)]\n",
    "\n",
    "            manhattan_list = manhattan_list + compare(df_to_find, df_to_check, manhattan_list, i)\n",
    "    \n",
    "    if (len(manhattan_list) > 0):\n",
    "        ts_corrected_manhattan = ts_correction(pd.DataFrame(manhattan_list, columns=[\"score\",\"start\",\"stop\",\"found in block\"]))\n",
    "        if (len(ts_corrected_manhattan) > 0):\n",
    "            thresholded_manhattan = calculate_threshold_value(pd.DataFrame(ts_corrected_manhattan, columns=[\"score\",\"start\",\"stop\",\"found in block\"]), 10, 1.1)\n",
    "            split_nr = df_to_find.index.size - 1\n",
    "                \n",
    "            split_manhatten = split(df_rm, pd.DataFrame(thresholded_manhattan, columns=[\"score\",\"start\",\"stop\",\"found in block\"]), split_nr)\n",
    "            \n",
    "            if split_manhatten.index.size > 3:\n",
    "                split_matches = cluster(split_manhatten, split_nr)\n",
    "            \n",
    "                type_overlap = [block_type]\n",
    "                \n",
    "                df_types = df_types.append(pd.DataFrame(data=[[str(blocks['start'].iloc[index]), str(blocks['stop'].iloc[index]), block_type]], columns=['start', 'stop', 'type']), ignore_index=True)\n",
    "                \n",
    "                for j in range(0, split_matches.index.size):\n",
    "                    if split_matches['Type'].iloc[j] == split_matches['Type'].iloc[0]:\n",
    "                        \n",
    "                        temp = df_rm.ix[split_matches['start'].iloc[j] : split_matches['stop'].iloc[j]]\n",
    "                        if max(temp.values) - min(temp.values) > 80.0: #AANGEPAST\n",
    "                            #plt.plot(temp)\n",
    "                            \n",
    "                            temp = pd.DataFrame(data= [[str(split_matches['start'].iloc[j]), str(split_matches['stop'].iloc[j]), block_type]], columns=['start', 'stop', 'type'])\n",
    "                            df_types = df_types.append(temp, ignore_index=True)\n",
    "                        \n",
    "                        # This is a match for df_to_find\n",
    "                        # assign a type to the block it was found in\n",
    "                        parent = split_matches['found in block'].iloc[j]\n",
    "                        found = False\n",
    "                        while not found:\n",
    "                            possible_block_matches = blocks.loc[blocks['nested_in'] == parent]\n",
    "                            temp_parent = check_overlap(possible_block_matches, split_matches['start'].iloc[j], split_matches['stop'].iloc[j])\n",
    "                            \n",
    "                            if temp_parent == -2:\n",
    "                                # no block found\n",
    "                                # assign type to temp_parent block\n",
    "                                if math.isnan(blocks['Type'].loc[parent]):\n",
    "                                    blocks['Type'].loc[parent] = block_type\n",
    "                                else:\n",
    "                                    type_overlap.append(blocks['Type'].loc[parent])\n",
    "                                \n",
    "                                found = True\n",
    "                            else:\n",
    "                                # parent block found, chick its children\n",
    "                                parent = temp_parent\n",
    "                \n",
    "                #plt.plot(df_rm.ix[blocks['start'].iloc[index] : blocks['stop'].iloc[index]])\n",
    "                #plt.show()\n",
    "                    \n",
    "    return block_type + 1, blocks, df_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_overlap(possible_list, start, stop, percentage = 0.9):\n",
    "    \"\"\"\n",
    "        Check if the matched piece clearly corresponds to a previously found block\n",
    "        \n",
    "        Args:\n",
    "            possible_list (pd.DataFrame) : list with possible matches\n",
    "            start (timestamp)\n",
    "            stop (timestamp)\n",
    "            \n",
    "        Returns\n",
    "            (int) : id of the corresponding block (or -2 if none)\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(0, possible_list.index.size):\n",
    "        \n",
    "        if pd.Timestamp(start) >= pd.Timestamp(possible_list['start'].iloc[i]) and pd.Timestamp(stop) <= pd.Timestamp(possible_list['stop'].iloc[i]):\n",
    "            return possible_list.index[i]\n",
    "        elif pd.Timestamp(start) >= pd.Timestamp(possible_list['start'].iloc[i]) and pd.Timestamp(stop) >= pd.Timestamp(possible_list['stop'].iloc[i]):\n",
    "            if (pd.Timestamp(possible_list['stop'].iloc[i]) - pd.Timestamp(start))/(pd.Timestamp(stop) - pd.Timestamp(start)) >= percentage:\n",
    "                return possible_list.index[i]\n",
    "        elif pd.Timestamp(start) <= pd.Timestamp(possible_list['start'].iloc[i]) and pd.Timestamp(stop) <= pd.Timestamp(possible_list['stop'].iloc[i]):\n",
    "            if (pd.Timestamp(stop) - pd.Timestamp(possible_list['start'].iloc[i]))/(pd.Timestamp(stop) - pd.Timestamp(start)) >= percentage:\n",
    "                return possible_list.index[i]\n",
    "            \n",
    "    return -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_duplicate(blocks, start, stop):\n",
    "    \"\"\"\n",
    "        Make sure a child block is different and not the same as its parent\n",
    "    \n",
    "        Args:\n",
    "            blocks (pd.DataFrame) : List of all previously found blocks\n",
    "            start (timestamp) : date & time when the found block started\n",
    "            stop (timestamp) : date & time when the found block ended\n",
    "            \n",
    "        Return:\n",
    "            True/ False\n",
    "            blocks (pd.DataFrame) : List of all previously found blocks\n",
    "    \"\"\"\n",
    "    \n",
    "    start = pd.Timestamp(start)\n",
    "    stop = pd.Timestamp(stop)\n",
    "    \n",
    "    \n",
    "    for i in range(0, blocks.index.size):\n",
    "        try :\n",
    "            check_start = pd.Timestamp(blocks['start'].iloc[i])\n",
    "            check_stop = pd.Timestamp(blocks['stop'].iloc[i])\n",
    "\n",
    "            if start == check_start or stop == check_stop or (start > check_start and stop < check_stop):\n",
    "                if (stop - start)/(check_stop - check_start) > 0.75:\n",
    "                    return False, blocks\n",
    "\n",
    "            if start < check_start and stop > check_stop:\n",
    "                if (check_stop - check_start)/(stop - start) > 0.75:\n",
    "                    blocks = blocks.drop(blocks.index[i])\n",
    "        except Exception as e:\n",
    "            print (e, \"i: \", i, \"blocks size: \", blocks.index.size)\n",
    "                \n",
    "            \n",
    "    return True, blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_types(df_types):\n",
    "    \"\"\"\n",
    "        If two types have a few exact same blocks they should be merged\n",
    "        \n",
    "        Args:\n",
    "            df_types (pd.DataFrame) : list with all types\n",
    "            \n",
    "        Returns:\n",
    "            df_types (pd.DataFrame) : list with all types\n",
    "    \"\"\"\n",
    "    \n",
    "    df_types = df_types.drop_duplicates(subset=['start', 'stop', 'type'], keep='first')\n",
    "\n",
    "    type_indexes = df_types['type'].drop_duplicates()\n",
    "\n",
    "    for i in range(0, type_indexes.index.size):\n",
    "        type_1 = type_indexes.iloc[i]\n",
    "\n",
    "        j = 0\n",
    "        while j < type_indexes.index.size:\n",
    "            type_2 = type_indexes.iloc[j]\n",
    "\n",
    "            size_1 = df_types[df_types['type'].isin([type_1, type_2])].index.size\n",
    "            size_2 = df_types[df_types['type'].isin([type_1, type_2])].drop_duplicates(subset=['start', 'stop'], keep='first').index.size\n",
    "\n",
    "            j = j + 1\n",
    "            if size_1 - size_2 >= 4:\n",
    "                df_types = df_types.replace(to_replace=[type_1, type_2], value=type_1).drop_duplicates(subset=['start', 'stop', 'type'], keep='first')\n",
    "                j = 0\n",
    "                \n",
    "    return df_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_small(df_types):\n",
    "    \"\"\"\n",
    "        Drop types that have insufficiant occurances\n",
    "        \n",
    "        Args:\n",
    "            df_types (pd.DataFrame) : list with all types\n",
    "            \n",
    "        Returns:\n",
    "            df_types (pd.DataFrame) : list with all types\n",
    "    \"\"\"\n",
    "    \n",
    "    # If a type has less than 4 occurances we discard the type\n",
    "    type_indexes = df_types['type'].drop_duplicates()\n",
    "    for i in range(0, type_indexes.index.size):\n",
    "        type_1 = type_indexes.iloc[i]\n",
    "\n",
    "        if df_types[df_types['type'] == type_1].index.size < 4:\n",
    "            df_types = df_types[df_types['type'] != type_1]\n",
    "            \n",
    "    return df_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reindex_types(df_types):\n",
    "    \"\"\"\n",
    "        Since some types have been merged or removed we want to reindex them\n",
    "        \n",
    "        Args:\n",
    "            df_types (pd.DataFrame) : list with all types\n",
    "            \n",
    "        Returns:\n",
    "            df_types (pd.DataFrame) : list with all types\n",
    "    \"\"\"\n",
    "    \n",
    "    type_indexes = df_types['type'].drop_duplicates()\n",
    "    for i in range(0, type_indexes.index.size):\n",
    "        index = type_indexes.iloc[i]\n",
    "        df_types = df_types.replace(to_replace=[index], value=i)\n",
    "        \n",
    "    return df_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def handle_duplicates(df_types):\n",
    "    \"\"\"\n",
    "        If two types have overlapping blocks they should be merged or one of them should be removed\n",
    "        \n",
    "        Args:\n",
    "            df_types (pd.DataFrame) : list with all types\n",
    "            \n",
    "        Returns:\n",
    "            df_types (pd.DataFrame) : list with all types\n",
    "    \"\"\"\n",
    "    df_merge = pd.DataFrame(columns=['type_1', 'type_2'])\n",
    "    df_blocks_to_merge = pd.DataFrame(columns=['first', 'start_1', 'stop_1', 'start_2', 'stop_2'])\n",
    "    \n",
    "    type_indexes = df_types['type'].drop_duplicates()\n",
    "\n",
    "    for i in range(0, type_indexes.index.size):\n",
    "        type_1 = type_indexes.iloc[i]\n",
    "\n",
    "\n",
    "        for j in range(i, type_indexes.index.size):\n",
    "            overlap = 0\n",
    "            type_2 = type_indexes.iloc[j]\n",
    "            \n",
    "            if type_1 != type_2:\n",
    "\n",
    "                list_1 = df_types[df_types['type'] == type_1]\n",
    "                list_2 = df_types[df_types['type'] == type_2]\n",
    "                for a in range(0, list_1.index.size):\n",
    "\n",
    "                    for b in range(0, list_2.index.size):\n",
    "\n",
    "                        if pd.Timestamp(list_1['start'].iloc[a]) >= pd.Timestamp(list_2['start'].iloc[b]) and pd.Timestamp(list_1['stop'].iloc[a]) <= pd.Timestamp(list_2['stop'].iloc[b]):\n",
    "                            # sample a of list 1 is completely inside sample b of list 2:\n",
    "                            # Remove sample a from list 1\n",
    "                            index_a = list_1.index[a]\n",
    "                            df_types = df_types[df_types.index != index_a]\n",
    "\n",
    "                        elif pd.Timestamp(list_1['start'].iloc[a]) <= pd.Timestamp(list_2['start'].iloc[b]) and pd.Timestamp(list_1['stop'].iloc[a]) >= pd.Timestamp(list_2['stop'].iloc[b]):\n",
    "                            # sample b of list 2 is completely inside sample a of list 1:\n",
    "                            # Remove sample b from list 2\n",
    "                            index_b = list_2.index[b]\n",
    "                            df_types = df_types[df_types.index != index_b]\n",
    "                            \n",
    "                        elif pd.Timestamp(list_1['start'].iloc[a]) <= pd.Timestamp(list_2['start'].iloc[b]) and pd.Timestamp(list_1['stop'].iloc[a]) <= pd.Timestamp(list_2['stop'].iloc[b]) and pd.Timestamp(list_1['stop'].iloc[a]) >= pd.Timestamp(list_2['start'].iloc[b]):\n",
    "                            #partial overlap\n",
    "                            if (pd.Timestamp(list_1['stop'].iloc[a]) - pd.Timestamp(list_2['start'].iloc[b])) / (pd.Timestamp(list_2['stop'].iloc[b]) - pd.Timestamp(list_1['start'].iloc[a])) >= 0.80:\n",
    "                                overlap = overlap + 1\n",
    "                                df_blocks_to_merge = df_blocks_to_merge.append(pd.DataFrame(data=[[type_1, list_1['start'].iloc[a], list_1['stop'].iloc[a], list_2['start'].iloc[b], list_2['stop'].iloc[b]]], columns=['first', 'start_1', 'stop_1', 'start_2', 'stop_2']))\n",
    "                                \n",
    "                        elif pd.Timestamp(list_2['start'].iloc[b]) <= pd.Timestamp(list_1['start'].iloc[a]) and pd.Timestamp(list_2['stop'].iloc[b]) <= pd.Timestamp(list_1['stop'].iloc[a]) and pd.Timestamp(list_2['stop'].iloc[b]) >= pd.Timestamp(list_1['start'].iloc[a]):\n",
    "                            #partial overlap\n",
    "                            if (pd.Timestamp(list_2['stop'].iloc[b]) - pd.Timestamp(list_1['start'].iloc[a])) / (pd.Timestamp(list_1['stop'].iloc[a]) - pd.Timestamp(list_2['start'].iloc[b])) >= 0.80:\n",
    "                                overlap = overlap + 1\n",
    "                                df_blocks_to_merge = df_blocks_to_merge.append(pd.DataFrame(data=[[type_2, list_2['start'].iloc[b], list_2['stop'].iloc[b], list_1['start'].iloc[a], list_1['stop'].iloc[a]]], columns=['first', 'start_1', 'stop_1', 'start_2', 'stop_2']))\n",
    "                                \n",
    "                    \n",
    "                if overlap > 3:\n",
    "                    # Merge Type_1 and Type_2\n",
    "                    df_merge = df_merge.append(pd.DataFrame(data=[[type_1, type_2]], columns=['type_1', 'type_2']),ignore_index = True)\n",
    "                    \n",
    "                    # Remove overlapping blocks and add a new block that wraps both of them\n",
    "                    for k in range(0, df_blocks_to_merge.index.size):\n",
    "                        if df_types[(df_types.start == df_blocks_to_merge['start_1'].iloc[k]) & (df_types.stop == df_blocks_to_merge['stop_1'].iloc[k])].index.size > 0:\n",
    "                            index_to_remove_1 = df_types[(df_types.start == df_blocks_to_merge['start_1'].iloc[k]) & (df_types.stop == df_blocks_to_merge['stop_1'].iloc[k])].index[0]\n",
    "                            df_types = df_types[df_types.index != index_to_remove_1]\n",
    "                        \n",
    "                        if df_types[(df_types.start == df_blocks_to_merge['start_2'].iloc[k]) & (df_types.stop == df_blocks_to_merge['stop_2'].iloc[k])].index.size > 0:\n",
    "                            index_to_remove_2 = df_types[(df_types.start == df_blocks_to_merge['start_2'].iloc[k]) & (df_types.stop == df_blocks_to_merge['stop_2'].iloc[k])].index[0]\n",
    "                            df_types = df_types[df_types.index != index_to_remove_2]\n",
    "                        \n",
    "                        df_types = df_types.append(pd.DataFrame(data=[[df_blocks_to_merge['start_1'].iloc[k], df_blocks_to_merge['stop_2'].iloc[k], df_blocks_to_merge['first'].iloc[k]]], columns=['start', 'stop', 'type']), ignore_index=True)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "    for i in range(0, df_merge.index.size):\n",
    "        df_types = df_types.replace(to_replace=[df_merge['type_2'].iloc[i]], value=df_merge['type_1'].iloc[i])\n",
    "    \n",
    "    return df_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_types_2(df_new_l,df):\n",
    "    amount_of_ind = df_new_l['type'].drop_duplicates()\n",
    "    for current_type in range(0, amount_of_ind.index.size):\n",
    "        if(df_new_l[df_new_l['type']==current_type].size > 0):\n",
    "            type_start = df_new_l[df_new_l['type']==current_type]['start'].iloc[0] #The first start_ts of the type\n",
    "            type_stop = df_new_l[df_new_l['type']==current_type]['stop'].iloc[0]  #The first stop_ts of the type\n",
    "            duration_type = pd.Timestamp(type_stop)-pd.Timestamp(type_start) #Duration of the type\n",
    "\n",
    "            total_consumption = df[type_start:type_stop].sum()[0]\n",
    "            for other_types in range(0, amount_of_ind.index.size):\n",
    "                if(df_new_l[df_new_l['type']==other_types].size > 0):\n",
    "                    type_start_other = df_new_l[df_new_l['type']==other_types]['start'].iloc[0]\n",
    "                    type_stop_other = df_new_l[df_new_l['type']==other_types]['stop'].iloc[0]\n",
    "                    duration_type_other = pd.Timestamp(type_stop_other)-pd.Timestamp(type_start_other) #Duration of the type\n",
    "                    total_consumption_other = df[type_start_other:type_stop_other].sum()[0]\n",
    "\n",
    "                    if(duration_type <= duration_type_other):\n",
    "                        ratio = duration_type/duration_type_other\n",
    "\n",
    "                    elif(duration_type > duration_type_other):\n",
    "                        ratio = duration_type_other/duration_type\n",
    "                    if(ratio >= 0.80):\n",
    "                        if(total_consumption <= total_consumption_other):\n",
    "                            ratio_2 = total_consumption/total_consumption_other\n",
    "                        elif(total_consumption > total_consumption_other):\n",
    "                            ratio_2 = total_consumption_other/total_consumption\n",
    "\n",
    "                        if(ratio_2 > 0.80):\n",
    "                            #Match types!\n",
    "#                             print \"matching type: \", current_type, \"with \", other_types\n",
    "                            df_new_l.loc[df_new_l['type'] == other_types, 'type'] = current_type\n",
    "                            amount_of_ind = df_new_l['type'].drop_duplicates()\n",
    "    return df_new_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#delete too long \n",
    "def del_too_long(df_new_l,ratio_=0.8):\n",
    "    amount_of_ind = df_new_l['type'].drop_duplicates()\n",
    "    for current_type in range(0, amount_of_ind.index.size):\n",
    "        duration_list=[]\n",
    "        if(df_new_l[df_new_l['type']==current_type].size > 0):\n",
    "            for current_index in range(0, df_new_l[df_new_l['type']==current_type].index.size):\n",
    "                type_start = df_new_l[df_new_l['type']==current_type]['start'].iloc[current_index] #The first start_ts of the type\n",
    "                type_stop = df_new_l[df_new_l['type']==current_type]['stop'].iloc[current_index]  #The first stop_ts of the type\n",
    "                duration_type = pd.Timestamp(type_stop)-pd.Timestamp(type_start) #Duration of the type\n",
    "                duration_list.append(duration_type)\n",
    "            #calculate median\n",
    "            median = np.median(duration_list)\n",
    "#             print \"duration_list= \", duration_list\n",
    "            for other_index in range(0, df_new_l[df_new_l['type']==current_type].index.size):\n",
    "#                 print \"other_index\", other_index\n",
    "                try:\n",
    "                    type_start = df_new_l[df_new_l['type']==current_type]['start'].iloc[other_index] #The first start_ts of the type\n",
    "                    type_stop = df_new_l[df_new_l['type']==current_type]['stop'].iloc[other_index]  #The first stop_ts of the type\n",
    "                    duration_type = pd.Timestamp(type_stop)-pd.Timestamp(type_start) #Duration of the type\n",
    "                    if(duration_type > median):\n",
    "                        ratio = median/duration_type\n",
    "                        if(ratio < ratio_):\n",
    "#                             print \"dropping: \", df_new_l[df_new_l['type'] ==current_type].iloc[other_index], \"duration: \", duration_type\n",
    "                            index_loc = df_new_l[df_new_l['type'] ==current_type].iloc[other_index].name\n",
    "                            location = df_new_l.index.get_loc(index_loc)\n",
    "#                             print \"test: \", df_new_l.index[location]\n",
    "                            df_new_l= df_new_l.drop(df_new_l.index[location])\n",
    "#                             df_new_l.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "                except Exception as e:\n",
    "                    print (e)\n",
    "\n",
    "\n",
    "    return df_new_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_start_end_points(df_types, df_list):\n",
    "#     df_list, df_rm = sax_rm_shortage_new(df, 20)\n",
    "    for i in range(0,df_types.index.size):\n",
    "        try:\n",
    "            current_start = pd.Timestamp(df_types.iloc[i]['start'])\n",
    "            current_stop = pd.Timestamp(df_types.iloc[i]['stop'])\n",
    "#\n",
    "            for j in range(0, df_list.index.size):\n",
    "                if(df_list.loc[current_start-pd.Timedelta(minutes=j)][0] < 2):\n",
    "                    current_start = current_start-pd.Timedelta(minutes=j)\n",
    "#\n",
    "                    df_types['start'].iloc[i] = current_start\n",
    "                    break\n",
    "#\n",
    "            for m in range(0, df_list.index.size):\n",
    "                if(df_list.loc[current_stop+pd.Timedelta(minutes=m)][0] < 2):\n",
    "                    current_stop= current_stop+pd.Timedelta(minutes=m)\n",
    "#\n",
    "                    df_types['stop'].iloc[i] = current_stop\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "    return df_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def find_types(blocks, df_rm, frame):\n",
    "    \"\"\"\n",
    "        The total pattern recognition functio\n",
    "        \n",
    "        Args:\n",
    "            blocks (pd.DataFrame) : list with all previously found blocks\n",
    "            df_rm (pd.DataFrame) : rolling mean of the original consumption\n",
    "            \n",
    "        Returns:\n",
    "            df_types (pd.DataFrame) : list of all types\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        del blocks['Type']\n",
    "        blocks = blocks.join(pd.DataFrame(columns=['Type']))\n",
    "    except Exception as e:\n",
    "        blocks = blocks.join(pd.DataFrame(columns=['Type']))\n",
    "\n",
    "    block_type = 0\n",
    "    df_types = pd.DataFrame(columns=['start', 'stop', 'type'])\n",
    "    \n",
    "    for i in range(0, blocks.index.size):\n",
    "\n",
    "        # Check if the block you want to find already has a type\n",
    "        if math.isnan(blocks['Type'].iloc[i]):\n",
    "            block_type, blocks, df_types = compare_all(i, blocks, df_rm, block_type, df_types)\n",
    "\n",
    "    # Clean up the found types\n",
    "    df_types = handle_duplicates(df_types)\n",
    "    \n",
    "    df_types = merge_types_2(df_types, df_rm)\n",
    "    \n",
    "    df_types = merge_types(df_types)\n",
    "    \n",
    "    df_types = drop_small(df_types)\n",
    "    \n",
    "    df_types = reindex_types(df_types)\n",
    "    \n",
    "    df_types = handle_duplicates(df_types)\n",
    "    \n",
    "    df_types = drop_small(df_types)\n",
    "    \n",
    "    df_types = reindex_types(df_types)\n",
    "    \n",
    "    df_types = get_start_end_points(df_types, frame)\n",
    "    \n",
    "    df_types = del_too_long(df_types)\n",
    "    \n",
    "    return df_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting functions #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_types(df_types, df_rm):\n",
    "    \"\"\"\n",
    "        Plot all found matches\n",
    "        \n",
    "        Args:\n",
    "            df_types (pd.DataFrame) : list containing all types\n",
    "            df_rm (pd.DataFrame) : rolling mean of original consumption\n",
    "    \"\"\"\n",
    "    \n",
    "    type_indexes = df_types['type'].drop_duplicates()\n",
    "    for i in range(0, type_indexes.index.size):\n",
    "        index = type_indexes.iloc[i]\n",
    "\n",
    "        temp = df_types[df_types['type'] == index]\n",
    "        for j in range(0, temp.index.size):\n",
    "            pot = df_rm.ix[temp['start'].iloc[j] : temp['stop'].iloc[j]]\n",
    "            plt.plot(pot) \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_coverage(df_types, df_rm):\n",
    "    \n",
    "    #plot the dataframe with all the blocks\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(df_rm.index, df_rm, color='grey')\n",
    "\n",
    "    colors = 100*['red', 'blue', 'orange', 'green', 'yellow', 'purple']\n",
    "\n",
    "    for i in range(0, df_types.index.size):\n",
    "        ax.axvspan(str(pd.Timestamp(df_types['start'].iloc[i])), str(pd.Timestamp(df_types['stop'].iloc[i])), alpha=0.1, color=colors[int(df_types['type'].iloc[i])])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non intrusive load monitoring function #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nilm(df_cons,factor,seconds=60, max_lvl=60):\n",
    "    try:\n",
    "        #block detection\n",
    "        frame, df_rm=sax_rm_shortage_new(df_cons, factor)\n",
    "        originalrm, blocks = blockdetection_new(frame, df_rm, seconds,max_lvl)\n",
    "\n",
    "        #type detection\n",
    "        df_types = find_types(blocks, df_rm, frame)\n",
    "        \n",
    "        #plot types\n",
    "        plot_types(df_types, df_rm)\n",
    "    except MyValidationError as exception:\n",
    "        # handle exception here and get error message\n",
    "        print(exception)\n",
    "    return df_types, df_rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_keuken, df_keuken_rm = nilm(df, 20, seconds=240, max_lvl=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}